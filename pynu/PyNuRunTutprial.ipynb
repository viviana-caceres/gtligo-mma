{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To start a PyCBC run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything you need to start your runs is in this directory **which you need to copy** to where you'd like to run from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cp /home/hannah.griggs/nu/pynu_tests/skyloc/run_testing/events_test/o3events/runmodule path/to/rundirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you copy that directory, you will need to adjust some of the files to point to your directories/namespace. To see all of the places in here that point to MY directory, run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "grep -i \"hannah.griggs\" *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should print all of the files that contain \"hannah.griggs\" and the line it's in. \n",
    "\n",
    "**You need to change the kinit and INI_LOC lines in run.sh (and runcache.sh)**. \n",
    "\n",
    "If you are running a plain HL job, no need to change statistic-files in analysis_dtdp_noinj.ini. If you are running a job with a custom dtdp pdf, that line needs to change with the location of the pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Launch the necessary environment (pynumods)**. Note that the environment command lives in \"environmentnew.sh\". This is just to make your life easier so you don't have to find the full path of the environment every time you want to launch it. \n",
    "\n",
    "You must deactivate the (igwn) environment first, then source the modified pynu environment like this with the \".\" followed by a space followed by the bash script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda deactivate # Get out of the (igwn) environment. Make sure to have no active environment at all before sourcing.\n",
    ". environmentnew.sh # Activate the pynu environment. The \". \" command tells your shell to accept the change for the remainder of the shell session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Running an analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running with reused data, see Part 2. If running with new data, START here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the runmodule directory are ini files for the pycbc workflow. To note here are:\n",
    "1. analysis_dtdp.ini (contains adjusted dtdp pdf input line, you'll want to edit this with your custum pdf)\n",
    "2. analysis_hl.ini (contains original dtdp PDF. Use this for INITIAL runs for NEW analyses.)\n",
    "3. inspiral.ini (contains injection-file, which controls which injection is used for this analysis)\n",
    "5. run.sh (the run script)\n",
    "6. runcache.sh (the run script for dtdp runs, has cache file option and the dtdp analysis file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **run.sh** will need to be edited as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "WORKFLOW_NAME=mmatest ## you can keep this the same\n",
    "CONFIG_TAG=v2.3.2.3  ## keep this\n",
    "GITLAB_URL=\"https://git.ligo.org/pycbc/offline-analysis/-/raw/${CONFIG_TAG}/production/o4/broad/config\"\n",
    "ID=injection ## Change this to indicate if it's a plain injection or a dtdp run\n",
    "RUNID=_150 ## Change to reflect the injected distance that goes with the dtdphase PDF you made AND any other info you need to keep track of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "INI_LOC=\"/home/hannah.griggs/nu/pynu_tests/skyloc/run_testing/events_test/o3events/\" # Change to your runmodule location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "kinit hannah.griggs ## Change to your ligo name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point your analysis to the correct injection file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Injection files live in my directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "/home/hannah.griggs/nu/banks/gen_plot_injections/autoinjections/pycbcinjs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The injections we're using have the following characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tc = 1239880055.862449 ## Central time that matches set of dtdp pdfs\n",
    "mass1 = 1.8 ## BNS\n",
    "mass2 = 1.5\n",
    "ra = 158.9333 ## Static sky location that matches set of dtdp pdfs\n",
    "dec = 46.4552\n",
    "inclination = 45.0\n",
    "coa_phase = 0.0\n",
    "polarization = 0.0\n",
    "f_ref = 20\n",
    "f_lower = 18\n",
    "approximant = IMRPhenomXHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each injection has a different distance parameter (50-350 Mpc). \n",
    "\n",
    "If you want to make a custom injection, see the injection tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit **inspiral.ini** with the correct injection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "injection-file = /home/hannah.griggs/nu/banks/gen_plot_injections/autoinjections/pycbcinjs/injectionsingle_150.hdf ## Change \"150\" to whichever injection you're working on. Make sure this matches your label in run.sh. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good to go! Run the analysis with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be prompted to enter your password, then it'll be off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Reusing matched filtering results from existing analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be tricky. The overarching steps are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the box you want to rerun.\n",
    "2. Copy over the mmatest-main.map file from that run directory to where you want to store your map files. In there should be the paths to all of the files that could be reused for another run of PyCBC\n",
    "3. Remove all files post-coinc step. That's a lot of files so you'll need to be careful about this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's break that down. First, open up the file and get ready to delete a bunch of lines. The lines we are removing represent all the output files from the coincident jobs and onward in the workflow. In other words, everything that depends on running pycbc_coinc_findtrigs. \n",
    "\n",
    "Things to keep in mind: \n",
    "- There is a logic to this file. It populates in order of job completion, so it's safe to assume that you want to keep everything before the first \"COINC\" output file shows up.\n",
    "- This file follows the structure of the main.dax files. If you want to double check anything, open up the dax file in the run directory and trace where the output of the coinc jobs go. \n",
    "\n",
    "#### **How to edit your file**\n",
    "1. Ctrl+F \"INSPIRAL\"\n",
    "2. Scroll to the end of each section of INSPIRAL outputs\n",
    "3. Delete EVERYTHING between INSPIRAL sections and after the last INSPIRAL section EXCEPT \n",
    "    - the FIT_OVER_PARAM, FIT_BY_TEMPLATE files.\n",
    "    - the H1L1-BANK2HDF_SPLITBANK files\n",
    "\n",
    "O4-specifc Plus:\n",
    "1. \"H1L1-EXCLUDE_ZEROLAG_FULL_DATA_2DET\"......hdf\n",
    "2. \"H1L1-FOREGROUND_CENSOR\".......xml\n",
    "3. \"H1L1-HDFINJFIND_NSBHSEOBNRV4_INJ_INJECTIONS\"........hdf\n",
    "4. \"H1L1-HDFINJFIND_ALL_INJECTIONS-1368975466\"........hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save this edited .map file under a unique name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit runcache.sh to point to the correct .map file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "  --cache-file maps/inputmap_inj120.map \\ ## Path that points to the .map file you want to use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using a custom DtDp PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit analysis_dtdp_noinj.ini to point to the custom PDF you are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "statistic-files = file://localhost/home/hannah.griggs/nu/pynu_tests/skyloc/dtdphase/L1H1-stat-O3_c1inj486_0.hdf  https://git.ligo.org/pycbc/offline-analysis/raw/v2.3.1.0/production/o4/kde_files/TEMPLATE_KDE_PYCBCO4_VERSION1.hdf  https://git.ligo.org/pycbc/offline-analysis/raw/v2.3.1.0/production/o4/kde_files/INJECTION_KDE_PYCBCO4_SNR_GT10_VERSION1.hdf ## Change the 1st file here to point to the PDF youre using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, **double check that all your other housekeeping is in order** in runcache.sh (Does the RUNID match your injection and dtdp pdf identifiers? Is your name in INI_LOC and kinit instead of mine?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good to go! Run the analysis with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./runcache.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be prompted to enter your password, then it'll be off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Troubleshooting if jobs are struggling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to babysit the jobs since they've been having issues with disk space.\n",
    "Check how the queue is doing from within the run directory with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If a small cluster of jobs fail**, let the analysis get as far as it can until the status updates to (FAILURE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restarting a job that failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it fails, edit the \"start\" script (in your run output directory) to include the preamble for the run.sh script (for authentication reasons):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ecp-get-cert --destroy\n",
    "htdestroytoken\n",
    "kinit hannah.griggs ## REMEMBER TO CHANGE TO YOUR NAME\n",
    "unset XDG_RUNTIME_DIR\n",
    "htgettoken -a vault.ligo.org -i igwn --scopes dqsegdb.read,gwdatafind.read,read:/frames,read:/ligo,read:/virgo,read:/kagra\n",
    "condor_vault_storer -v igwn\n",
    "export GWDATAFIND_SERVER=\"datafind.ligo.org:443\"\n",
    "PEGASUS_PYTHON=/home/ian.harry/conda_envs/pegasus_python/bin/python PATH=/home/ian.harry/conda_envs/pegasus_python/bin/:${PATH}\n",
    "\n",
    "pegasus-run /local/hannah.griggs/pycbc-tmp_u_1hqa3g/work $@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can restart the job with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restarting a job that's held"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If jobs are getting held**, see the reason with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "condor_q better-analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will tell you which job requirements are insufficient and by how much. If memory or disk space are the problem, update held jobs like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "condor_qedit -constraint \"JOBSTATUS==5\" RequestDisk=newrequestamount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change RequestDisk to RequestMemory as needed, and only request a little over what the jobs seem to need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Release jobs again with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "condor_release -constraint \"JOBSTATUS==5\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
