{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To start a PyCBC run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything you need to start your runs is in this directory **which you need to copy** to where you'd like to run from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cp -r /home/hannah.griggs/nu/pynu_tests/o2grbs/pynuruns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you copy that directory, you will need to adjust some of the files to point to your directories/namespace. To see all of the places in here that point to MY directory, run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "grep -i \"hannah.griggs\" *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should print all of the files that contain \"hannah.griggs\" and the line it's in. Yay grep! \n",
    "\n",
    "**You only need to change the INI_LOC and STATISTIC_FILE lines in runhlo2.sh**. \n",
    "\n",
    "The STATISTIC_FILE should be changed to the location of the custom dtdp PDF that you made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Launch the necessary environment (pynumods)**.\n",
    "\n",
    "You must deactivate the (igwn) environment first, then source the modified pynu environment like this with the \"source\" command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda deactivate # Get out of the (igwn) environment. Make sure to have no active environment at all before sourcing.\n",
    "source src/nu-dev/pynumods/bin/activate # Activate the pynu environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting up your GRB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the O2 rerun experiments, we are rerunning the O2 PyCBC boxes with targeted TPA PDFs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather your GRB information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit https://docs.google.com/spreadsheets/d/1FuLUsVUoQGJPYha1vU2znyChYIt1mCyerwVhiiYxeBU/edit?usp=sharing to see the list of GRBs for O2. This contains the information you will use to generate the sky-phase-amplitude PDFs, following the TPA PDF tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the PDF is generated, identify which PyCBC offline chunk the GRB GPS time falls into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk times are gathered into this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "/home/hannah.griggs/nu/pynu_tests/o2grbs/chunkinis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which I also copied into pynuruns. The files within are actually the full configurations for the offline O2 PyCBC runs, but to see the GPS time span for each, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "head ch2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing `ch2` with the chunk number you want to look at. The `head` command prints the first 10 lines of a file. Likewise, `tail` prints the last 10 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you find the correct chunk, note the start and end times. You will use these for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Running an analysis with reused data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pynuruns directory are ini files for the pycbc workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **runhlo2.sh** will need to be edited as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "WORKFLOW_NAME=mmatest ## you can keep this the same\n",
    "CONFIG_TAG=v2.3.2.3  ## keep this\n",
    "GITLAB_URL=\"https://git.ligo.org/pycbc/offline-analysis/-/raw/${CONFIG_TAG}/production/o4/broad/config\"\n",
    "ID=grb161210524 ## Change this to your GRB name\n",
    "RUNID=_4 ## If this is a rerun, use this line to indicate which rerun. Otherwise this can be blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "INI_LOC=\"/home/hannah.griggs/nu/pynu_tests/o2grbs/\" # Change to your pynuruns location\n",
    "STATISTIC_FILE=\"home/hannah.griggs/nu/pynu_tests/skyloc/dtdphase/L1H1-stat-GRB161212652.hdf\" # Change to your custom PDC location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To rerun an analysis using existing results, we need to use a **cache file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `maps` directory in pynuruns, you will notice a file called `chunk2.map`. This is an example of a cache file that tells PyCBC the jobs it doesn't need to redo. In that file are two `HDF_TRIGGER_MERGE` files, one for each IFO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the GPS start and end times you identified for the PyCBC chunk corresponding to your GRB, locate the `HDF_TRIGGER_MERGE` file that matches the chunk times in this archive directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "/home/ian.harry/aLIGO/O2/analyses/ALL_TRIGGER_FILES/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the file to your `chunkinis` directory to make transferring more efficient (by trial and error, it seems like transfers succeed more often if the file is in your namespace):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cp /home/ian.harry/aLIGO/O2/analyses/ALL_TRIGGER_FILES/H1-HDF_TRIGGER_MERGE_FULL_DATA-1164556817-1929600.hdf chunkinis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the `chunk2.map` cache file and name it after the chunk you are working with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files have entries which tell PyCBC where to find the files it can be reused, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "L1-HDF_TRIGGER_MERGE_FULL_DATA-1164556817-1929600.hdf /home/hannah.griggs/nu/pynu_tests/o2grbs/chunkinis/L1-HDF_TRIGGER_MERGE_FULL_DATA-1164556817-1929600.hdf pool=\n",
    "\"local\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace the GPS times in the file with those matching the `HDF_TRIGGER_MERGE` file you found.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit `runhlo2.sh` to point to your cache file instead of `chunk2.map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "  --cache-file maps/chunk2.map \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you happen to need chunk 2, then these steps are done for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good to go! Run the analysis with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./runhlo2.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be prompted to enter your password, then it'll be off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Troubleshooting if jobs are struggling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to babysit the jobs since they've been having issues with disk space.\n",
    "Check how the queue is doing from within the run directory with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If a small cluster of jobs fail**, let the analysis get as far as it can until the status updates to (FAILURE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restarting a job that failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it fails, edit the \"start\" script (in your run output directory) to include the preamble for the run.sh script (for authentication reasons):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ecp-get-cert --destroy\n",
    "htdestroytoken\n",
    "kinit hannah.griggs ## REMEMBER TO CHANGE TO YOUR NAME\n",
    "unset XDG_RUNTIME_DIR\n",
    "htgettoken -a vault.ligo.org -i igwn --scopes dqsegdb.read,gwdatafind.read,read:/frames,read:/ligo,read:/virgo,read:/kagra\n",
    "condor_vault_storer -v igwn\n",
    "export GWDATAFIND_SERVER=\"datafind.ligo.org:443\"\n",
    "PEGASUS_PYTHON=/home/ian.harry/conda_envs/pegasus_python/bin/python PATH=/home/ian.harry/conda_envs/pegasus_python/bin/:${PATH}\n",
    "\n",
    "pegasus-run /local/hannah.griggs/pycbc-tmp_u_1hqa3g/work $@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can restart the job with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restarting a job that's held"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If jobs are getting held**, see the reason with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "condor_q better-analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will tell you which job requirements are insufficient and by how much. If memory or disk space are the problem, update held jobs like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "condor_qedit -constraint \"JOBSTATUS==5\" RequestDisk=newrequestamount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change RequestDisk to RequestMemory as needed, and only request a little over what the jobs seem to need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Release jobs again with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "condor_release -constraint \"JOBSTATUS==5\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
